{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a21fd2",
   "metadata": {},
   "source": [
    "# Workflow for predicting contract data.\n",
    "### You can place contract pdf files in the Test_Files folder and recieve the contract's name, parties, effective date, and agreement date. A test set of contracts has been placed in the directory for ease of use.\n",
    "## Make sure all of the needed packages are installed and run the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f52aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\users\\adria\\anaconda3\\lib\\site-packages (1.22.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\adria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\adria\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\adria\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\adria\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (1.22.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\adria\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\adria\\anaconda3\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\adria\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\adria\\anaconda3\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\adria\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\adria\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\adria\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\adria\\appdata\\roaming\\python\\python310\\site-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\adria\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adria\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\adria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\adria\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\adria\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\adria\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.22.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adria\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.1.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adria\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\adria\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->spacy) (3.0.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adria\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\adria\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\adria\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#install required packages\n",
    "!pip install PyMuPDF\n",
    "!pip install datasets\n",
    "!pip install spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e92150a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "import os, re, math, random, json, string, csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification, PreTrainedModel, RobertaTokenizerFast\n",
    "\n",
    "from datasets import load_dataset, ClassLabel, Sequence \n",
    "\n",
    "import fitz\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "#resolve any conflicting libraries\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c8e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SEED FOR REPRODUCIBILITY\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# BATCH SIZE\n",
    "# IDEALLY USE SAME BATCH SIZE FOR INFERENCE AS WAS USED FOR TRAINING\n",
    "BATCH_SIZES = 2\n",
    "\n",
    "# WHICH PRE-TRAINED TRANSFORMER TO FINE-TUNE?\n",
    "MODEL_CHECKPOINT = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04335e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_CLASS_LABELS = \"feature_class_labels.json\"\n",
    "TEMP_MODEL_OUTPUT_DIR = 'temp_model_output_dir'\n",
    "SAVED_MODEL = f\"C964v2-NER-Fine-Tune-Transformer-Final-{MODEL_CHECKPOINT}\" # Change for notebook version\n",
    "TEST_FILE_PATH = \"Test_Files/\"\n",
    "TEST_DATA_FILE = 'test_data_file.json'\n",
    "CSV_DATA_FILE = 'legal_agreement_data_file.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e885e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 5 legal agreements from Test_Files/ folder:  ['CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement.pdf', 'CybergyHoldingsInc_20140520_10-Q_EX-10.27_8605784_EX-10.27_Affiliate Agreement.pdf', 'DigitalCinemaDestinationsCorp_20111220_S-1_EX-10.10_7346719_EX-10.10_Affiliate Agreement.pdf', 'LinkPlusCorp_20050802_8-K_EX-10_3240252_EX-10_Affiliate Agreement.pdf', 'SouthernStarEnergyInc_20051202_SB-2A_EX-9_801890_EX-9_Affiliate Agreement.pdf']\n"
     ]
    }
   ],
   "source": [
    "#walk through PDF files and create dataframe with the names of the files\n",
    "pdf_files = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(TEST_FILE_PATH):\n",
    "    pdf_files.extend(filenames)\n",
    "#remove any hidden files in directory\n",
    "for i, f in enumerate(pdf_files):\n",
    "    if f.startswith(\".\"):\n",
    "        pdf_files.pop(i)\n",
    "print(f\"Uploaded {len(pdf_files)} legal agreements from {TEST_FILE_PATH} folder: \", pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "590d0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text cleaning function for standard PDF parsing workflow\n",
    "def pre_process_doc_common(text):\n",
    "    text = text.replace(\"\\n\", \" \")  #replacement for \"\\n\"   \n",
    "    text = text.replace(\"\\xa0\", \" \")  #replacement for \"\\xa0\"\n",
    "    text = text.replace(\"\\x0c\", \" \")  #replacement for \"\\x0c\"\n",
    "    \n",
    "    regex = \"\\ \\.\\ \"\n",
    "    subst = \".\"\n",
    "    text = re.sub(regex, subst, text, 0)  #remove multiple dots\n",
    "        \n",
    "    regex = \"_\"\n",
    "    subst = \" \"\n",
    "    text = re.sub(regex, subst, text, 0)  #remove underscores\n",
    "       \n",
    "    regex = \"--+\"\n",
    "    subst = \" \"\n",
    "    text = re.sub(regex, subst, text, 0)   #remove multiple dashes\n",
    "        \n",
    "    regex = \"\\*+\"\n",
    "    subst = \"*\"\n",
    "    text = re.sub(regex, subst, text, 0)  #remove multiple stars\n",
    "        \n",
    "    regex = \"\\ +\"\n",
    "    subst = \" \"\n",
    "    text = re.sub(regex, subst, text, 0)  #remove multiple whitespace\n",
    "    \n",
    "    text = text.strip()  #remove whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f54433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function takes in file list, reads each file, cleans the text and returns all agreements in a list\n",
    "def text_data(test_dir, pdf_files, print_text=False, clean_text=True, max_len=3000):\n",
    "    text_list = []\n",
    "    for filename in tqdm(pdf_files):\n",
    "        agreement = fitz.open(test_dir+filename)\n",
    "        full_text = \"\"\n",
    "        for page in agreement:\n",
    "            full_text += page.get_text('text')#+\"\\n\"\n",
    "        if print_text:\n",
    "            print(\"Text before cleaning: \\n\", full_text)\n",
    "\n",
    "        #run text through cleansing function\n",
    "        if clean_text:\n",
    "            full_text = pre_process_doc_common(full_text)\n",
    "        short_text = full_text[:max_len]\n",
    "        len_text = len(short_text)\n",
    "\n",
    "        if print_text:\n",
    "            print(\"Text after cleaning: \\n\", short_text)\n",
    "\n",
    "        text_list.append([filename, full_text, short_text, len_text])\n",
    "        \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c801d0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 22.02it/s]\n"
     ]
    }
   ],
   "source": [
    "#call cleaning functions on list of PDF files in testing folder\n",
    "test_dir = TEST_FILE_PATH\n",
    "data = text_data(test_dir, pdf_files, print_text=False, clean_text=True, max_len=1000)\n",
    "\n",
    "#create dataframe with text\n",
    "columns = ['File_Name','Full_Text', 'Short_Text', 'Length_Of_Short_Text']\n",
    "text_df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a534b28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/adria/.cache/huggingface/datasets/json/default-7582f4fb73183989/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1883304dcab4417a823827a64514ec85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fe604afd334350a7f073e67cb1c476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/adria/.cache/huggingface/datasets/json/default-7582f4fb73183989/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7fc50f6d5a4a1a86cc83846754ba91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['dummy_ner_tags', 'split_tokens'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#tokenize each agreement prior to bringing into the transformer model\n",
    "#create tokens using spaCy\n",
    "nlp = English()\n",
    "text_df['tokens'] = text_df['Short_Text'].apply(lambda x: nlp(x))\n",
    "\n",
    "#split tokens into a list ready for CSV\n",
    "text_df['split_tokens'] = text_df['tokens'].apply(lambda x: [tok.text for tok in x])\n",
    "\n",
    "#create dummy NER tags for alignment\n",
    "text_df['dummy_ner_tags'] = text_df['tokens'].apply(lambda x: [0 for tok in x])\n",
    "\n",
    "#serialise data to JSON for archive\n",
    "export_columns = ['split_tokens', 'dummy_ner_tags']\n",
    "export_df = text_df[export_columns]\n",
    "export_df.to_json(TEST_DATA_FILE, orient=\"table\", index=False)\n",
    "text_df = text_df.drop(['dummy_ner_tags'], axis=1)\n",
    "\n",
    "#re-import serialized JSON data and create dataset in transformer format\n",
    "data_files = TEST_DATA_FILE\n",
    "datasets = load_dataset('json', data_files=data_files, field='data')\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9780124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open label list that was created in pre-processing\n",
    "with open(FEATURE_CLASS_LABELS, 'r') as f:\n",
    "    label_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "579655de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5da2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that executes roBERTa tokenizer and aligns the tokens with the labels\n",
    "def tokenize_and_align_labels(examples, label_all_tokens=False):\n",
    "    tokenized_inputs = tokenizer(examples[\"split_tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"dummy_ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            #special tokens have word_idx = set label to -100 to ignore in loss function\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            #set label for first token of each word\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            #for other tokens in a word, set the label to either current label or -100 based on label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34a9feaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#use the map method of our dataset object to apply tokenize_and_align_labels to the training and validation sets\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True, load_from_cache_file=True)\n",
    "\n",
    "#dataset produces warning when using cached files, pass load_from_cache_file=False to preprocess again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66300702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model and instantiate\n",
    "loaded_model = AutoModelForTokenClassification.from_pretrained('C964v2-NER-Fine-Tune-Transformer-roberta-base')\n",
    "\n",
    "#define model arguments\n",
    "args = TrainingArguments(output_dir = TEMP_MODEL_OUTPUT_DIR,\n",
    "                         per_device_train_batch_size=BATCH_SIZES,\n",
    "                         per_device_eval_batch_size=BATCH_SIZES,\n",
    "                         seed=RANDOM_SEED\n",
    "                        )\n",
    "\n",
    "#data collator makes batches samples and makes them all the same size\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "#instantiate predictor\n",
    "pred_trainer = Trainer(\n",
    "    loaded_model,\n",
    "    args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8bdffc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: dummy_ner_tags, split_tokens. If dummy_ner_tags, split_tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5\n",
      "  Batch size = 2\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#extract predictions\n",
    "predictions, labels, _ = pred_trainer.predict(tokenized_datasets[\"train\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "text_df['predictions'] = list(predictions)\n",
    "\n",
    "#remove special tokens\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "text_df['true_predictions'] = true_predictions\n",
    "\n",
    "#consolidate information into DataFrame\n",
    "def data_extract(tuple_list):\n",
    "    de_list = []\n",
    "    for tup in tuple_list:\n",
    "        if tup[1] != 'O':\n",
    "            de_list.append(tup)\n",
    "    return de_list\n",
    "\n",
    "text_df['check_pred'] = list(list(zip(a,b)) for a,b in zip(text_df['split_tokens'], text_df['true_predictions']))\n",
    "text_df['data_tuples'] = text_df['check_pred'].apply(data_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667aa110",
   "metadata": {},
   "source": [
    "### Running the code below will allow the user to see the values that the model predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ef49139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>agmt_name</th>\n",
       "      <th>agmt_parties</th>\n",
       "      <th>agmt_date</th>\n",
       "      <th>eff_date</th>\n",
       "      <th>Full_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CreditcardscomInc_20070810_S-1_EX-10.33_362297...</td>\n",
       "      <td>CHASE AFFILIATE AGREEMENT</td>\n",
       "      <td>[Chase Bank USA , N.A., Chase, an, “, Affiliat...</td>\n",
       "      <td>April 6 , 2007</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Exhibit 10.33 Last Updated: April 6, 2007 CHAS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CybergyHoldingsInc_20140520_10-Q_EX-10.27_8605...</td>\n",
       "      <td>Marketing Affiliate Agreement</td>\n",
       "      <td>[Birch First Global Investments Inc., Mount Kn...</td>\n",
       "      <td>8th day of May 2014</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Exhibit 10.27 MARKETING AFFILIATE AGREEMENT Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DigitalCinemaDestinationsCorp_20111220_S-1_EX-...</td>\n",
       "      <td>NETWORK AFFILIATE AGREEMENT</td>\n",
       "      <td>[DIGITAL CINEMA DESTINATIONS CORP ., National ...</td>\n",
       "      <td>14th day of March , 2011</td>\n",
       "      <td>N/A</td>\n",
       "      <td>DIGITAL CINEMA DESTINATIONS CORP. NETWORK AFFI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinkPlusCorp_20050802_8-K_EX-10_3240252_EX-10_...</td>\n",
       "      <td>AFFILIATE AGREEMENT</td>\n",
       "      <td>[Link Plus Corporation, Axiometric , LLC, Axio...</td>\n",
       "      <td>JULY 15 , 2005</td>\n",
       "      <td>N/A</td>\n",
       "      <td>EXHIBIT 10.1 AFFLIATE AGREEMENT DATED JULY 15,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SouthernStarEnergyInc_20051202_SB-2A_EX-9_8018...</td>\n",
       "      <td>Affiliate Program Management and Conditions</td>\n",
       "      <td>[]</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Exhibit 10.8 Affiliate Program / Premium Affil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File_Name  \\\n",
       "0  CreditcardscomInc_20070810_S-1_EX-10.33_362297...   \n",
       "1  CybergyHoldingsInc_20140520_10-Q_EX-10.27_8605...   \n",
       "2  DigitalCinemaDestinationsCorp_20111220_S-1_EX-...   \n",
       "3  LinkPlusCorp_20050802_8-K_EX-10_3240252_EX-10_...   \n",
       "4  SouthernStarEnergyInc_20051202_SB-2A_EX-9_8018...   \n",
       "\n",
       "                                     agmt_name  \\\n",
       "0                    CHASE AFFILIATE AGREEMENT   \n",
       "1                Marketing Affiliate Agreement   \n",
       "2                  NETWORK AFFILIATE AGREEMENT   \n",
       "3                          AFFILIATE AGREEMENT   \n",
       "4  Affiliate Program Management and Conditions   \n",
       "\n",
       "                                        agmt_parties  \\\n",
       "0  [Chase Bank USA , N.A., Chase, an, “, Affiliat...   \n",
       "1  [Birch First Global Investments Inc., Mount Kn...   \n",
       "2  [DIGITAL CINEMA DESTINATIONS CORP ., National ...   \n",
       "3  [Link Plus Corporation, Axiometric , LLC, Axio...   \n",
       "4                                                 []   \n",
       "\n",
       "                  agmt_date eff_date  \\\n",
       "0            April 6 , 2007      N/A   \n",
       "1       8th day of May 2014      N/A   \n",
       "2  14th day of March , 2011      N/A   \n",
       "3            JULY 15 , 2005      N/A   \n",
       "4                       N/A      N/A   \n",
       "\n",
       "                                           Full_Text  \n",
       "0  Exhibit 10.33 Last Updated: April 6, 2007 CHAS...  \n",
       "1  Exhibit 10.27 MARKETING AFFILIATE AGREEMENT Be...  \n",
       "2  DIGITAL CINEMA DESTINATIONS CORP. NETWORK AFFI...  \n",
       "3  EXHIBIT 10.1 AFFLIATE AGREEMENT DATED JULY 15,...  \n",
       "4  Exhibit 10.8 Affiliate Program / Premium Affil...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#functions to extract each important data point based on the model's labeling of tokens\n",
    "def extract_agreement_date(tuple_list):\n",
    "    temp_date=0\n",
    "    for d in tuple_list:\n",
    "        if d[1] == \"B-AGMT_DATE\":\n",
    "            temp_date=d[0]\n",
    "        elif d[1] == \"I-AGMT_DATE\":\n",
    "            temp_date = temp_date + \" \" + d[0]\n",
    "        else:\n",
    "            continue\n",
    "    if temp_date == 0:\n",
    "        return 'N/A'\n",
    "    return temp_date\n",
    "\n",
    "text_df['agmt_date'] = text_df['data_tuples'].apply(extract_agreement_date)\n",
    "\n",
    "def extract_effective_date(tuple_list):\n",
    "    temp_date=0\n",
    "    for d in tuple_list:\n",
    "        if d[1] == \"B-EFF_DATE\":\n",
    "            temp_date=d[0]\n",
    "        elif d[1] == \"I-EFF_DATE\":\n",
    "            temp_date = temp_date + \" \" + d[0]\n",
    "        else:\n",
    "            continue\n",
    "    if temp_date == 0:\n",
    "        return 'N/A'\n",
    "    return temp_date\n",
    "\n",
    "text_df['eff_date'] = text_df['data_tuples'].apply(extract_effective_date)\n",
    "\n",
    "def extract_agreement_name(tuple_list):\n",
    "    for n in tuple_list:\n",
    "        if n[1] == \"B-DOC_NAME\":\n",
    "            temp_name=n[0]\n",
    "        elif n[1] == \"I-DOC_NAME\":\n",
    "            temp_name = temp_name + \" \" + n[0]\n",
    "        else:\n",
    "            continue\n",
    "    return temp_name\n",
    "\n",
    "text_df['agmt_name'] = text_df['data_tuples'].apply(extract_agreement_name)\n",
    "\n",
    "def extract_agreement_parties(tuple_list):\n",
    "    data_dict = defaultdict(list)\n",
    "    for i, p in enumerate(tuple_list):\n",
    "        if p[1] == \"B-PARTY\":\n",
    "            temp_party=p[0]\n",
    "            if i == (len(tuple_list)-1):\n",
    "                data_dict[\"Parties\"].append(temp_party)\n",
    "            elif tuple_list[i+1][1] != \"I-PARTY\":\n",
    "                data_dict[\"Parties\"].append(temp_party)\n",
    "        elif p[1] == \"I-PARTY\":\n",
    "            temp_party = temp_party + \" \" + p[0]\n",
    "            if i == (len(tuple_list)-1):\n",
    "                data_dict[\"Parties\"].append(temp_party)\n",
    "            elif tuple_list[i+1][1] != \"I-PARTY\":\n",
    "                data_dict[\"Parties\"].append(temp_party)\n",
    "\n",
    "    return list(dict.fromkeys(data_dict['Parties']))\n",
    "\n",
    "text_df['agmt_parties'] = text_df['data_tuples'].apply(extract_agreement_parties)\n",
    "\n",
    "#create and sort dataframe with needed information\n",
    "export_df = text_df[['File_Name', 'agmt_name','agmt_parties','agmt_date','eff_date', 'Full_Text']].copy()\n",
    "export_df = export_df.sort_values('File_Name', axis=0)\n",
    "\n",
    "#display predictions dataframe\n",
    "export_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55207eb",
   "metadata": {},
   "source": [
    "### Formatted display of first contract in dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48d76728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: \t\t CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement.pdf\n",
      "Agreement Name: \t CHASE AFFILIATE AGREEMENT\n",
      "Agreement Date: \t April 6 , 2007\n",
      "Agreement Date: \t N/A\n",
      "Agreement Parties:\n",
      "\t\t\t Chase Bank USA , N.A.\n",
      "\t\t\t Chase\n",
      "\t\t\t an\n",
      "\t\t\t “\n",
      "\t\t\t Affiliate\n",
      "\t\t\t Chase.com\n"
     ]
    }
   ],
   "source": [
    "#display first sample\n",
    "sample=0\n",
    "print(\"File Name: \\t\\t\",export_df.iloc[sample][0])\n",
    "print(\"Agreement Name: \\t\",export_df.iloc[sample][1])\n",
    "print(\"Agreement Date: \\t\",export_df.iloc[sample][3])\n",
    "print(\"Agreement Date: \\t\",export_df.iloc[sample][4])\n",
    "print(\"Agreement Parties:\")\n",
    "for p in export_df.iloc[sample][2]:\n",
    "    print(\"\\t\\t\\t\", p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
